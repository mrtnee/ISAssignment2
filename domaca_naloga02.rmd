---
title: "[IS] Domača naloga 2"
author: "Martin Preradovič, Niki Bizjak"
date: "9 12 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Cleaning
Before doing anything with our text, we have to clean it. The text that we receive consists of unicode characters, escape symbols, HTML tags, etc. Reading the data using `tsvread` does not work, because the text is incorrectly escaped, so we must read it using the `fread` function. After reading tables from disk, we convert training and test labels to factors. Finally, we load custom stopwords list.

```{r message=FALSE, warning=FALSE}
library(tm)
library(ggplot2)
require(data.table)

# Read training and test data from disk
train_data = fread('./insults/train.tsv', sep = '\t', header = TRUE, quote = '')
test_data = fread('./insults/test.tsv', sep = '\t', header = TRUE, quote = '')

# The label is either 0 or 1, so we can convert it to factor
train_data$label <- as.factor(train_data$label)
test_data$label <- as.factor(test_data$label)

# Read list of english stopwords
stopwords_file = file("./data/stopwords.txt", open="r")
custom_stopwords = readLines(stopwords_file)
close(stopwords_file)
```

After all the data is loaded, we create a corpus source and apply corpus operations on it. We first remove all unicode characters and escape sequences, remove stopwords, remove everything that is not a leter and strip whitespace.
We are left with ascii text, but it contains multiple different versions of each word (for example `going`, `goes` and `go`). To remove this we could lemmatise our corpus, but for the sake of speed, we will simply stem our corpus - so remove the suffixes of words.

```{r warning=FALSE}
train_corpus <- Corpus(VectorSource(train_data$text_a))
test_corpus <- Corpus(VectorSource(test_data$text_a))

# Remove unicode characters and "escape" characters
patterns = c("\\\\x[0-9|a-f]{2}", "\\\\u[0-9|a-f]{4}", "\\\\[:alpha:]")
for (pattern in patterns) {
  train_corpus <- tm_map(train_corpus, content_transformer(gsub),
                         pattern = pattern, replacement = ' ')
  test_corpus <- tm_map(test_corpus, content_transformer(gsub),
                        pattern = pattern, replacement = ' ')
}

# Remove english stopwords and stem them
train_corpus <- tm_map(train_corpus, removeWords, stopwords('english'))
train_corpus <- tm_map(train_corpus, removeWords, custom_stopwords)
train_corpus <- tm_map(train_corpus, stemDocument)

# Remove everything that is not a letter (so not [:alpha:])
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)

test_corpus <- tm_map(test_corpus, removePunctuation)
test_corpus <- tm_map(test_corpus, removeNumbers)

test_corpus <- tm_map(test_corpus, removeWords, stopwords('english'))
test_corpus <- tm_map(test_corpus, removeWords, custom_stopwords)
test_corpus <- tm_map(test_corpus, stemDocument)

# Remove whitespace and convert to lowercase
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, content_transformer(tolower))

test_corpus <- tm_map(test_corpus, stripWhitespace)
test_corpus <- tm_map(test_corpus, content_transformer(tolower))
```

## Exploration

### Word frequencies
Now that we have cleaned our text, we can plot the frequency of words in our corpus.
```{r fig.height=3}
tdm <- TermDocumentMatrix(train_corpus)

# Calculate word frequency for each word in term-document matrix
term_frequency <- rowSums(as.matrix(tdm))

# Only plot words that appear at least 150 times in all documents
most_frequent <- subset(term_frequency, term_frequency >= 150)
barplot(most_frequent)
```
As we can see from our plot, the most common words are `like`, `you` `get`, `the`, `fuck`, `one` `know`, `make`, `think`, `peopl`, which are all commonly used English words. The word that stands out a bit is `fuck`, but this is understandable as we are working with insults dataset.

### Clustering
For clustering we will use the k-means clustering. The document-term matrix is very sparse, which means that there are a lot of zeros in our matrix. We could remove the most sparse terms in our matrix, but we have decided to only use the most common words to do the clustering.

```{r warning=FALSE}
# Get only the 50 most used words from our term_frequency matrix
most_used_words = names(sort(term_frequency, decreasing = TRUE)[1:60])

# First, we construct a document - term matrix
dtm <- DocumentTermMatrix(train_corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)

# The matrix mat is very sparse - meaning that there is a lot of empty space
# Because clustering is slow, we will only use the rows from the 50 most used words
mat <- mat[,most_used_words]

# Perform clustering for k = {2, 4, 8, 16}
for (number_of_clusters in c(2, 4, 8, 16)) {
  cat(sprintf("number_of clusters = %d\n", number_of_clusters))
  
  # Use the k-means clustering with number of clusters
  kmeans_result <- kmeans(mat, number_of_clusters)
  
  # Find the most popular words in all clusters
  for (i in 1:number_of_clusters) {
	  kmeans_result_sorted <- sort(kmeans_result$centers[i,], decreasing=T)
	  cat(names(kmeans_result_sorted)[1:10], "\n")
  }
  
  cat("\n");
}
```

### Projection to two dimensions

To project our documents to two dimensions we used both the PCA and t-SNE algorithm. The colors of the plot represent the training label of a document. The t-SNE visualisation is better, as we can more clearly see the individual document clusters in our plot - as we can see, we have one big cluster and many smaller ones.

```{r fig.height=4, message=FALSE, warning=FALSE}
# First, project our documents in two dimensions using PCA
pca <- prcomp(mat)
qplot(pca$x[,1], pca$x[,2], color = train_data$label)

# Then use the t-SNE to perform projection
library(Rtsne)
tsne <- Rtsne(mat, perplexity=20, theta=0.2, dims=2, check_duplicates = F)
qplot(tsne$Y[,1], tsne$Y[,2], color = train_data$label)
```

### POS tagging
```{r}
library(NLP)
library(openNLP)

sentence_annotator <- Maxent_Sent_Token_Annotator()
word_annotator <- Maxent_Word_Token_Annotator()
pos_annotator <- Maxent_POS_Tag_Annotator()

```

## Modeling

### Class label distribution
```{r}
summary(train_data$label)
```
The label distribution in our training (and for that matter testing) data is very unbalanced. There are only a quarter texts, that are labeled as insults.

### Classification
For the classification, we have selected the *k-nearest neighbors*, *naive Bayes* and *boosted logistic regression* algorithms, as they are easy to understand, very fast and work pretty well with natural language processing problems.

We have also used the *Linear Support Vector Machines* model, because it is believed that it works extremly well with natural language processing problems.

For the performance metric we chose the *accuracy*, *precision*, *recall*, *f1 score* and *specificity*, because they are easy to calculate and give a good metric of the performance of our models.

```{r warning=FALSE}
library(caret)
library(MLmetrics)

# Create a table from document-term matrix and labels in the end
model_train_data <- data.frame(mat, train_data$label)

test_dtm <- DocumentTermMatrix(test_corpus, control = list(weighting=weightTfIdf))
test_mat <- as.matrix(test_dtm)
test_mat <- test_mat[,most_used_words]

# possible_models = names(getModelInfo())
possible_methods = c("knn", "naive_bayes", "LogitBoost", "svmLinearWeights")


# Create a control for our model training - we will use repeated cross validation
control <- trainControl(method = 'repeatedcv', number = 10, repeats = 3)

cat('| Classification model |   Accuracy |  Precision |     Recall |   F1 Score | Specificity|\n');

for (method in possible_methods) {
  # Train the model using the training data and repeated cv training control
  model <- train(
    train_data.label ~ .,
    data = model_train_data,
    method=method,
    trControl=control
  )
  
  # To predict many results at once, use the extractPrediction function from carret
  model_predictions = extractPrediction(
    list(mode_name=model),
    testX = test_mat,
    testY = as.factor(test_data$label),
  )
  
  # The returned matrix has the following columns: obs, pred, model, dataType, object
  # The obs is the observed data type, pred the predicted
  # We are only interested in test data, so we only need elements where dataType is "Test"
  predictions = model_predictions[model_predictions$dataType=='Test',]
  
  # We can now use this data to calculate model accuracy, precision, specificity, etc.
  accuracy = Accuracy(predictions$obs, predictions$pred)
  precision = Precision(predictions$obs, predictions$pred)
  recall = Recall(predictions$obs, predictions$pred)
  f1_score = F1_Score(predictions$obs, predictions$pred)
  specificity <- Specificity(predictions$obs, predictions$pred)
  
  cat(sprintf("| %20s |     %1.4f |     %1.4f |     %1.4f |     %1.4f |     %1.4f |\n",
              method, accuracy, precision, recall, f1_score, specificity))
}
```

## Understanding

Perform feature ranking by using different filter methods and display the top 10 features for each filter method.

``` {r warning=FALSE}
library(CORElearn)

# First, we construct a document - term matrix
dtm <- DocumentTermMatrix(train_corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)

relief.importances <- sort(attrEval(train_data.label ~ ., model_train_data, "Relief"), decreasing = T)
feature.names.relief <- names(relief.importances)
feature.names.relief[1:10]

eqrelieff.importances <- sort(attrEval(train_data.label ~ ., model_train_data, "ReliefFequalK"), decreasing = T)
feature.names.eqrelieff <- names(eqrelieff.importances)
feature.names.eqrelieff[1:10]

exprelieff.importances <- sort(attrEval(train_data.label ~ ., model_train_data, "ReliefFexpRank"), decreasing = T)
feature.names.exprelieff <- names(exprelieff.importances)
feature.names.exprelieff[1:10]

bestKRelieff.importances <- sort(attrEval(train_data.label ~ ., model_train_data, "ReliefFbestK"), decreasing = T)
feature.names.bestKRelieff <- names(bestKRelieff.importances)
feature.names.bestKRelieff[1:10]

infGain.importances <- sort(attrEval(train_data.label ~ ., model_train_data, "InfGain"), decreasing = T)
feature.names.infGain <- names(infGain.importances)
feature.names.infGain[1:10]

gini.importances <- sort(attrEval(train_data.label ~ ., model_train_data, "Gini"), decreasing = T)
feature.names.gini <- names(gini.importances)
feature.names.gini[1:10]

mdl.importances <- sort(attrEval(train_data.label ~ ., model_train_data, "MDL"), decreasing = T)
feature.names.mdl <- names(mdl.importances)
feature.names.mdl[1:10]

cosine.importances <- sort(attrEval(train_data.label ~ ., model_train_data, "DistAngle"), decreasing = T)
feature.names.cosine <- names(cosine.importances)
feature.names.cosine[1:10]

reliefFMerit.importances <- sort(attrEval(train_data.label ~ ., model_train_data, "ReliefFmerit"), decreasing = T)
feature.names.reliefFMerit <- names(reliefFMerit.importances)
feature.names.reliefFMerit[1:10]

rReliefFSqrDist.importances <- sort(attrEval(train_data.label ~ ., model_train_data, "MyopicReliefF"), decreasing = T)
feature.names.rReliefFSqrDist <- names(rReliefFSqrDist.importances)
feature.names.rReliefFSqrDist[1:10]

```

### Model re-evaluation with top n number of features

``` {r warning = FALSE}
nofFeatures <- c(1, 2, 3, 5, 7, 9, 15, 20, 30, 40)

method = "svmLinearWeights";

control <- trainControl(method = 'repeatedcv', number = 10, repeats = 3)


cat('| Number of top features |   Accuracy |  Precision |     Recall |   F1 Score | Specificity|\n');
for (n in nofFeatures) {
  # First extract a new dtm with only the top n features
  top_train_dtm = dtm[,intersect(colnames(dtm), feature.names.rReliefFSqrDist[1:n])]
  mat <- as.matrix(top_train_dtm)
  
  top_test_dtm <- DocumentTermMatrix(test_corpus)
  top_test_dtm <- top_test_dtm[, intersect(colnames(top_test_dtm), feature.names.rReliefFSqrDist[1:n])]
  test_mat <- as.matrix(top_test_dtm)
  
  model_train_data <- data.frame(mat, train_data$label)
  
  # Train the model using the training data and repeated cv training control
  model <- train(
    train_data.label ~ .,
    data = model_train_data,
    method=method,
    trControl=control
  )
  
  # To predict many results at once, use the extractPrediction function from carret
  model_predictions = extractPrediction(
    list(mode_name=model),
    testX = test_mat,
    testY = as.factor(test_data$label),
  )
  
  predictions = model_predictions[model_predictions$dataType=='Test',]
  
  accuracy = Accuracy(predictions$obs, predictions$pred)
  precision = Precision(predictions$obs, predictions$pred)
  recall = Recall(predictions$obs, predictions$pred)
  f1_score = F1_Score(predictions$obs, predictions$pred)
  specificity <- Specificity(predictions$obs, predictions$pred)
  
  cat(sprintf("| %22d |     %1.4f |     %1.4f |     %1.4f |     %1.4f |     %1.4f |\n",
              n, accuracy, precision, recall, f1_score, specificity))
}


```






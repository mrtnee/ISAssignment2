---
title: "[IS] Domača naloga 2"
author: "Martin Preradovič, Niki Bizjak"
date: "9 12 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Cleaning

```{r warning=FALSE}
library(tm)
library(ggplot2)
require(data.table)

# Read training and test data from disk
train_data = fread('./insults/train.tsv', sep = '\t', header = TRUE, quote = '')
test_data = fread('./insults/test.tsv', sep = '\t', header = TRUE, quote = '')

# The label is either 0 or 1, so we can convert it to factor
train_data$label <- as.factor(train_data$label)
test_data$label <- as.factor(test_data$label)

# Read list of english stopwords
stopwords_file = file("./data/stopwords.txt", open="r")
custom_stopwords = readLines(stopwords_file)
close(stopwords_file)
```

```{r warning=FALSE}
train_corpus <- Corpus(VectorSource(train_data$text_a))
test_corpus <- Corpus(VectorSource(test_data$text_a))

# Our data contains unicode characters (like \u... and \x... and escape sequences \n \r, ...), remove them
patterns = c("\\\\x[0-9|a-f]{2}", "\\\\u[0-9|a-f]{2}");
for (pattern in patterns) {
  train_corpus <- tm_map(train_corpus, content_transformer(gsub), pattern = pattern, replacement = ' ')
  test_corpus <- tm_map(test_corpus, content_transformer(gsub), pattern = pattern, replacement = ' ')
}

# Remove english stopwords and stem them
train_corpus <- tm_map(train_corpus, removeWords, stopwords('english'))
train_corpus <- tm_map(train_corpus, removeWords, custom_stopwords)
train_corpus <- tm_map(train_corpus, stemDocument)

test_corpus <- tm_map(test_corpus, removeWords, stopwords('english'))
test_corpus <- tm_map(test_corpus, removeWords, custom_stopwords)
test_corpus <- tm_map(test_corpus, stemDocument)

# Remove everything that is not a letter (so not [:alpha:])
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, content_transformer(tolower))

test_corpus <- tm_map(test_corpus, removePunctuation)
test_corpus <- tm_map(test_corpus, removeNumbers)
test_corpus <- tm_map(test_corpus, stripWhitespace)
test_corpus <- tm_map(test_corpus, content_transformer(tolower))
```


## Exploration

### Word frequencies
```{r}
tdm <- TermDocumentMatrix(train_corpus)

# Calculate word frequency for each word in term-document matrix
term_frequency <- rowSums(as.matrix(tdm))

# Only plot words that appear at least 150 times in all documents
most_frequent <- subset(term_frequency, term_frequency >= 150)
barplot(most_frequent)
```

### Clustering
```{r warning=FALSE}
# Get only the 100 most used words from our term_frequency matrix
most_used_words = names(sort(term_frequency, decreasing = TRUE)[1:100])

# First, we construct a document - term matrix
dtm <- DocumentTermMatrix(train_corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)

# The matrix mat is very sparse - meaning that there is a lot of empty space
# Because clustering is slow, we will only use the rows from the 100 most used words
mat <- mat[, most_used_words]

# Perform clustering for k = {2, 4, 8, 16}
for (number_of_clusters in c(2, 4, 8, 16)) {
  cat(sprintf("number_of clusters = %d\n", number_of_clusters))
  
  # Use the k-means clustering with number of clusters
  kmeans_result <- kmeans(mat, number_of_clusters)
  
  # Find the most popular words in all clusters
  for (i in 1:number_of_clusters) {
	  kmeans_result_sorted <- sort(kmeans_result$centers[i,], decreasing=T)
	  cat(names(kmeans_result_sorted)[1:10], "\n")
  }
  
  cat("\n");
}
```

### Projection to two dimensions
```{r}
# First, project our documents in two dimensions using PCA
pca <- prcomp(mat)
qplot(pca$x[,1], pca$x[,2], color = train_data$label)

# Then use the t-SNE to perform projection
library(Rtsne)
tsne <- Rtsne(mat, perplexity=20, theta=0.2, dims=2, check_duplicates = F)
qplot(tsne$Y[,1], tsne$Y[,2], color = train_data$label)
```
